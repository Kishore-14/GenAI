{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import contextlib\n",
    "import io\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read data\n",
    "\n",
    "df = pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"# this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000# the maximum for text-embedding-ada-002 is 8191\n",
    "top_n = 1000\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "col_embedding = 'embedding'\n",
    "n_tsne=2\n",
    "n_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Gets the embedding from OpenAI\n",
    "def get_embedding(text, model):\n",
    "    openai.api_key = \"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\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return  openai.Embedding.create(input= [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "col_txt = 'Text'\n",
    "df[\"n_tokens\"] = df[col_txt].apply(lambda x: len(encoding.encode(x)))\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "df = df[df.n_tokens > 0].reset_index(drop=True) ##Remove if there no tokens, for example blank lines\n",
    "# Get embeddings one by one\n",
    "texts = df[col_txt].tolist()  # Convert Series to a list of strings\n",
    "\n",
    "# Iterate through each text and get embeddings\n",
    "embeddings = [get_embedding(text, model='text-embedding-ada-002') for text in texts]\n",
    "\n",
    "# Assign embeddings to the DataFrame\n",
    "df[col_embedding] = embeddings\n",
    "\n",
    "matrix = np.array(df[col_embedding].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "max_tokens = 512  # Maximum tokens for BERT\n",
    "top_n = 1000\n",
    "n_clusters = 5  # Number of clusters\n",
    "n_tsne = 2\n",
    "n_iter = 1000\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text and get BERT embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_tokens, truncation=True, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Preprocess data\n",
    "df[\"n_tokens\"] = df[\"Text\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "df = df[df.n_tokens > 0].reset_index(drop=True)  # Remove if there are no tokens\n",
    "\n",
    "# Get BERT embeddings\n",
    "df[\"embedding\"] = df[\"Text\"].apply(get_embedding)\n",
    "matrix = np.array(df[\"embedding\"].to_list())\n",
    "\n",
    "# Make clustering\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans_clusters = kmeans_model.fit_predict(matrix)\n",
    "\n",
    "# TSNE\n",
    "tsne_model = TSNE(n_components=n_tsne, verbose=0, random_state=42, n_iter=n_iter, init='random')\n",
    "tsne_out = tsne_model.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kmeans_cluster\"] = kmeans_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kmeans_cluster\"] = df[\"kmeans_cluster\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming df contains your DataFrame with columns \"Scores\" and \"kmeans_cluster\"\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(df[\"Score\"], df[\"kmeans_cluster\"])\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Assuming matrix contains your data matrix and kmeans_clusters are the cluster labels\n",
    "# Calculate silhouette score\n",
    "silhouette = silhouette_score(matrix, kmeans_clusters)\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "\n",
    "# Calculate Davies-Bouldin index\n",
    "db_index = davies_bouldin_score(matrix, kmeans_clusters)\n",
    "print(\"Davies-Bouldin Index:\", db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already computed the t-SNE embeddings in tsne_out\n",
    "# tsne_out should be a numpy array of shape (n_samples, 2)\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_out[:, 0], tsne_out[:, 1], c=kmeans_clusters, cmap='viridis')\n",
    "plt.title('t-SNE Visualization with KMeans Clusters')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
